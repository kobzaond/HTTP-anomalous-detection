{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse as urlparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import mixture\n",
    "from scipy.cluster.vq import whiten\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_file_raw = 'normalTrafficTraining.txt'\n",
    "normalTest_file_raw = 'normalTrafficTest.txt'\n",
    "anomaly_file_raw = 'anomalousTrafficTest.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset and process features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_name):\n",
    "    file = open(file_name)\n",
    "    doc = file.readlines()\n",
    "\n",
    "    parsed_requests = []\n",
    "    for i in range(len(doc)):\n",
    "        line = doc[i].strip()\n",
    "        content_length = 0\n",
    "        if line.startswith(\"GET\"):\n",
    "            parsed_requests.append(\"GET: \" + line.split(\" \")[1] + \" \" + str(content_length))\n",
    "        elif line.startswith(\"POST\") or line.startswith(\"PUT\"):\n",
    "            url = line.split(' ')[0] + \" \" + line.split(' ')[1]\n",
    "            j = 1\n",
    "            while True:\n",
    "                if doc[i + j].startswith(\"Content-Length\"):\n",
    "                    string = doc[i + j].split(' ')\n",
    "                    content_length = string[1]\n",
    "                    break\n",
    "                j += 1\n",
    "            j += 1\n",
    "            data = doc[i + j + 1].strip()\n",
    "            url += '?' + data\n",
    "            parsed_requests.append(url + \" \" + content_length)\n",
    "\n",
    "    file.close()\n",
    "\n",
    "    data_set = np.zeros((len(parsed_requests), 12))\n",
    "    counter = 0\n",
    "\n",
    "    for request in parsed_requests:\n",
    "\n",
    "        url = request.split(\" \")\n",
    "        content_length = int(url[2])\n",
    "        parsed = urlparse.urlparse(url[1])\n",
    "        path = parsed.path\n",
    "        request_arguments = list(urlparse.parse_qs(parsed.query).values())\n",
    "\n",
    "        length_of_arguments = 0\n",
    "        number_of_arguments = len(request_arguments)\n",
    "        length_of_query = len(parsed.query)\n",
    "        number_of_letter_chars_in_path = sum(c.isalpha() for c in set(list(parsed.query)))\n",
    "\n",
    "        number_of_digits_in_arguments = 0\n",
    "        number_of_letters_in_arguments = 0\n",
    "        number_of_special_chars_in_args = 0\n",
    "        arg_lens = [0]\n",
    "        equal_char = 0\n",
    "        for a in request_arguments:\n",
    "            numbers = sum(c.isdigit() for c in a[0])\n",
    "            letters = sum(c.isalpha() for c in a[0])\n",
    "            number_of_digits_in_arguments += numbers\n",
    "            number_of_letters_in_arguments += letters\n",
    "            number_of_special_chars_in_args += sum(not c.isdigit() and not c.isalpha() for c in a[0])\n",
    "            length_of_arguments += len(a[0])\n",
    "            arg_lens.append(len(a[0]))\n",
    "            equal_char += sum(c == \"=\" for c in a[0])\n",
    "\n",
    "        max_arg_len = max(arg_lens)\n",
    "        if not number_of_arguments == 0:\n",
    "            avg_arg_len = length_of_arguments / number_of_arguments\n",
    "        else:\n",
    "            avg_arg_len = 0\n",
    "\n",
    "        length_of_path = len(path)\n",
    "        path = path.replace(\"/\", \"\")\n",
    "        length_of_request = length_of_query + length_of_path\n",
    "        number_of_special_chars_in_path = sum(not c.isdigit() and not c.isalpha() for c in set(list(path)))\n",
    "\n",
    "        data_set[counter] = np.array(\n",
    "            [max_arg_len, number_of_arguments, content_length,  number_of_digits_in_arguments,\n",
    "             number_of_special_chars_in_args, length_of_arguments,  number_of_letters_in_arguments,\n",
    "             length_of_request,  number_of_special_chars_in_path, avg_arg_len, number_of_letter_chars_in_path,\n",
    "             length_of_path]\n",
    "        )\n",
    "\n",
    "        counter += 1\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_and_f1(y_test, y_pred):\n",
    "    TN, FP, FN, TP = confusion_matrix(y_test, y_pred).ravel()\n",
    "    Precision = (TP * 1.0) / (TP + FP)\n",
    "    Recall = (TP * 1.0) / (TP + FN)\n",
    "    ACC = (TP + TN) * 1.0 / (TP + TN + FP + FN)\n",
    "    F1 = 2.0 * Precision * Recall / (Precision + Recall)\n",
    "    return [ACC, F1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = load_dataset(normal_file_raw)\n",
    "testset_good = load_dataset(normalTest_file_raw)\n",
    "testset_bad = load_dataset(anomaly_file_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data whitening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kobzaond/.local/lib/python3.6/site-packages/scipy/cluster/vq.py:140: RuntimeWarning: Some columns have standard deviation zero. The values of these columns will not change.\n",
      "  RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "train_set = whiten(train_set)\n",
    "testset_good = whiten((testset_good))\n",
    "testset_bad = whiten(testset_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do parameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_test = train_test_split(train_set, [0]*train_set.shape[0], test_size=0.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_prev = 0\n",
    "n_components = 1\n",
    "for number_of_components in range(1, 10):\n",
    "    gmm = mixture.GaussianMixture(n_components=number_of_components, covariance_type='full').fit(X_train)\n",
    "    pred = gmm.score_samples(X_train)\n",
    "    treshold = np.min(pred)\n",
    "    misclasses = 0\n",
    "    FP = 1\n",
    "    FN = 0\n",
    "    TP = 1\n",
    "    TN = 0\n",
    "    for sample in X_val:\n",
    "        pred3 = gmm.score_samples(sample.reshape(1, -1))\n",
    "        if pred3 < treshold:\n",
    "            misclasses += 1\n",
    "            FN += 1\n",
    "        else:\n",
    "            TN += 1\n",
    "    Precision = (TP * 1.0) / (TP + FP)\n",
    "    Recall = (TP * 1.0) / (TP + FN)\n",
    "    F1 = (2*Precision*Recall)/(Precision+Recall)\n",
    "    if F1<=F1_prev:\n",
    "        n_components = number_of_components\n",
    "        break\n",
    "    else:\n",
    "        F1_prev = F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of mixture components:  3\n"
     ]
    }
   ],
   "source": [
    "print(\"number of mixture components: \", n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learn distribution of normal requests (EM algorithm is used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = mixture.GaussianMixture(n_components=n_components, covariance_type='full').fit(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select treshold : the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = gmm.score_samples(train_set)\n",
    "treshold = np.min(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute TP, TN, FP, FN, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclasses = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "TP =0\n",
    "TN = 0\n",
    "max_pred = treshold\n",
    "for sample in testset_bad:\n",
    "    pred2 = gmm.score_samples(sample.reshape(1, -1))\n",
    "    if pred2 > treshold:\n",
    "        misclasses += 1\n",
    "        FP += 1\n",
    "    else:\n",
    "        TP += 1\n",
    "    if pred2 > max_pred:\n",
    "        max_pred = pred2\n",
    "\n",
    "for sample in testset_good:\n",
    "    pred3 = gmm.score_samples(sample.reshape(1, -1))\n",
    "    if pred3 < treshold:\n",
    "        misclasses += 1\n",
    "        FN += 1\n",
    "    else:\n",
    "        TN += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print information about the classifier performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of misclassifications  0\n",
      "accuracy:  1.0\n",
      "F1  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"number of misclassifications \", misclasses)\n",
    "print(\"accuracy: \", 1 - misclasses / (testset_good.shape[0] + testset_good.shape[0]))\n",
    "Precision = (TP * 1.0) / (TP + FP)\n",
    "Recall = (TP * 1.0) / (TP + FN)\n",
    "print(\"F1 \", (2*Precision*Recall)/(Precision+Recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One should get accuracy and F1 measure in range [0.999,1.0], where 1.0 indicates perfect classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other option is to use a surpevised classifier, as suggested:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate all three datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.concatenate((testset_bad, train_set, testset_good))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make target labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "yBad = [1] * testset_bad.shape[0]\n",
    "yGood = [0] * (train_set.shape[0] + testset_good.shape[0])\n",
    "y = yBad + yGood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide data onto training set and test set. 60% in traing set, 40% in test set\n",
    "#There are 25065 samples of anomalous http requests, all datasets contain together 97065 samples,\n",
    "#40% of all data means 38826.0 samples. So in both datasets, training and test set will be samples from both classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train, X_test, y_train, y_test = train_test_split(all_data, y, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a surpevised logistic regression classifier:\n",
    "#Logistic regression is a simple classifier, but it can provide us a baseline and also give us an information\n",
    "#whether the classification problem is linearly separable or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgs = LogisticRegression()\n",
    "lgs.fit(X_train, y_train)\n",
    "y_pred = lgs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ACC, F1] = get_acc_and_f1(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show accuracy and F1 measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9961195054945055  F1: 0.9923550504025438\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy: \", ACC, \" F1:\", F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As the accuracy and F1 are over 0.99, it indicates, that the two classes are linearly separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With SVM using a linear kernel, it is possible to get 100% accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.LinearSVC()\n",
    "parameters = {'C': [0.001, 0.1, 1,  100, 1000]}\n",
    "clf = GridSearchCV(svc, parameters, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tune the hyperparameter C with 10-fold crossvalidation using grid search algorithm\n",
    "#and classify the samples in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ACC, F1] = get_acc_and_f1(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  1.0  F1: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy: \", ACC, \" F1:\", F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM: The primal classification problem is transformed onto a dual problem (Lagrangian duality).\n",
    "#The dual problem is a convex optimization problem and leads to Quadratic Programming.\n",
    "#For a QP problems, it is always possible to find a global optima."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
