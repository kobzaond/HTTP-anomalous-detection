{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse as urlparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import mixture\n",
    "from scipy.cluster.vq import whiten\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_file_raw = 'normalTrafficTraining.txt'\n",
    "normalTest_file_raw = 'normalTrafficTest.txt'\n",
    "anomaly_file_raw = 'anomalousTrafficTest.txt'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Load dataset and process features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_name):\n",
    "    file = open(file_name)\n",
    "    doc = file.readlines()\n",
    "\n",
    "    parsed_requests = []\n",
    "    for i in range(len(doc)):\n",
    "        line = doc[i].strip()\n",
    "        content_length = 0\n",
    "        if line.startswith(\"GET\"):\n",
    "            parsed_requests.append(\"GET: \" + line.split(\" \")[1] + \" \" + str(content_length))\n",
    "        elif line.startswith(\"POST\") or line.startswith(\"PUT\"):\n",
    "            url = line.split(' ')[0] + \" \" + line.split(' ')[1]\n",
    "            j = 1\n",
    "            while True:\n",
    "                if doc[i + j].startswith(\"Content-Length\"):\n",
    "                    string = doc[i + j].split(' ')\n",
    "                    content_length = string[1]\n",
    "                    break\n",
    "                j += 1\n",
    "            j += 1\n",
    "            data = doc[i + j + 1].strip()\n",
    "            url += '?' + data\n",
    "            parsed_requests.append(url + \" \" + content_length)\n",
    "\n",
    "    file.close()\n",
    "\n",
    "    data_set = np.zeros((len(parsed_requests), 12))\n",
    "    counter = 0\n",
    "\n",
    "    for request in parsed_requests:\n",
    "\n",
    "        url = request.split(\" \")\n",
    "        content_length = int(url[2])\n",
    "        parsed = urlparse.urlparse(url[1])\n",
    "        path = parsed.path\n",
    "        request_arguments = list(urlparse.parse_qs(parsed.query).values())\n",
    "\n",
    "        length_of_arguments = 0\n",
    "        number_of_arguments = len(request_arguments)\n",
    "        length_of_query = len(parsed.query)\n",
    "        number_of_letter_chars_in_path = sum(c.isalpha() for c in set(list(parsed.query)))\n",
    "\n",
    "        number_of_digits_in_arguments = 0\n",
    "        number_of_letters_in_arguments = 0\n",
    "        number_of_special_chars_in_args = 0\n",
    "        arg_lens = [0]\n",
    "        equal_char = 0\n",
    "        for a in request_arguments:\n",
    "            numbers = sum(c.isdigit() for c in a[0])\n",
    "            letters = sum(c.isalpha() for c in a[0])\n",
    "            number_of_digits_in_arguments += numbers\n",
    "            number_of_letters_in_arguments += letters\n",
    "            number_of_special_chars_in_args += sum(not c.isdigit() and not c.isalpha() for c in a[0])\n",
    "            length_of_arguments += len(a[0])\n",
    "            arg_lens.append(len(a[0]))\n",
    "            equal_char += sum(c == \"=\" for c in a[0])\n",
    "\n",
    "        max_arg_len = max(arg_lens)\n",
    "        if not number_of_arguments == 0:\n",
    "            avg_arg_len = length_of_arguments / number_of_arguments\n",
    "        else:\n",
    "            avg_arg_len = 0\n",
    "\n",
    "        length_of_path = len(path)\n",
    "        path = path.replace(\"/\", \"\")\n",
    "        length_of_request = length_of_query + length_of_path\n",
    "        number_of_special_chars_in_path = sum(not c.isdigit() and not c.isalpha() for c in set(list(path)))\n",
    "\n",
    "        data_set[counter] = np.array(\n",
    "            [max_arg_len, number_of_arguments, content_length,  number_of_digits_in_arguments,\n",
    "             number_of_special_chars_in_args, length_of_arguments,  number_of_letters_in_arguments,\n",
    "             length_of_request,  number_of_special_chars_in_path, avg_arg_len, number_of_letter_chars_in_path,\n",
    "             length_of_path]\n",
    "        )\n",
    "\n",
    "        counter += 1\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_and_f1(y_test, y_pred):\n",
    "    TN, FP, FN, TP = confusion_matrix(y_test, y_pred).ravel()\n",
    "    Precision = (TP * 1.0) / (TP + FP)\n",
    "    Recall = (TP * 1.0) / (TP + FN)\n",
    "    ACC = (TP + TN) * 1.0 / (TP + TN + FP + FN)\n",
    "    F1 = 2.0 * Precision * Recall / (Precision + Recall)\n",
    "    return [ACC, F1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = load_dataset(normal_file_raw)\n",
    "testset_good = load_dataset(normalTest_file_raw)\n",
    "testset_bad = load_dataset(anomaly_file_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data whitening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kobzaond/.local/lib/python3.6/site-packages/scipy/cluster/vq.py:140: RuntimeWarning: Some columns have standard deviation zero. The values of these columns will not change.\n",
      "  RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "train_set = whiten(train_set)\n",
    "testset_good = whiten((testset_good))\n",
    "testset_bad = whiten(testset_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do parameter tuning:\n",
    "Because after whitening the features are decorrelated, the covariance matrices should be diagonal.\n",
    "Also, for each hyperparameter setting, it should be trained several times (7), because of parameters random initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_set, [0]*train_set.shape[0], test_size=0.35) \n",
    "ACC_prev = 0\n",
    "comp_num = 1\n",
    "for number_of_components in range(1, 100):\n",
    "    best_ACC = 0\n",
    "    for i in range(0, 7):\n",
    "        gmm = mixture.GaussianMixture(n_components=number_of_components, covariance_type='diag').fit(X_train)\n",
    "        predict = gmm.score_samples(X_train)\n",
    "        treshold = np.min(predict)\n",
    "        misclasses = 0\n",
    "        for sample in X_val:\n",
    "            predict_val = gmm.score_samples(sample.reshape(1, -1))\n",
    "            if predict_val < treshold:\n",
    "                misclasses += 1\n",
    "        ACC = 1 - (misclasses / X_val.shape[0])\n",
    "        if best_ACC < ACC:\n",
    "            best_ACC = ACC\n",
    "    if best_ACC <= ACC_prev:\n",
    "        break\n",
    "    else:\n",
    "        ACC_prev = best_ACC\n",
    "        comp_num = number_of_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of mixture components:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"number of mixture components: \", comp_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn distribution of normal requests (EM algorithm is used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = mixture.GaussianMixture(n_components=comp_num, covariance_type='diag').fit(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select treshold : the least normal sample from normal samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = gmm.score_samples(train_set)\n",
    "treshold = np.min(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute TP, TN, FP, FN, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = gmm.score_samples(train_set)\n",
    "treshold = np.min(predict)  \n",
    "misclasses = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "TP =0\n",
    "TN = 0\n",
    "max_pred = treshold\n",
    "for sample in testset_bad:\n",
    "    predict_sample = gmm.score_samples(sample.reshape(1, -1))\n",
    "    if predict_sample > treshold:\n",
    "        misclasses += 1\n",
    "        FP += 1\n",
    "    else:\n",
    "        TP += 1\n",
    "    if predict_sample > max_pred:\n",
    "        max_pred = predict_sample\n",
    "    \n",
    "for sample in testset_good:\n",
    "    predict_val = gmm.score_samples(sample.reshape(1, -1))\n",
    "    if predict_val < treshold:\n",
    "        misclasses += 1\n",
    "        FN += 1\n",
    "    else:\n",
    "        TN += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print information about the classifier performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of misclassifications  1\n",
      "accuracy:  0.9999861111111111\n",
      "F1  0.9999800522630707\n"
     ]
    }
   ],
   "source": [
    "print(\"number of misclassifications \", misclasses)\n",
    "print(\"accuracy: \", 1 - misclasses / (testset_good.shape[0] + testset_good.shape[0]))\n",
    "Precision = (TP * 1.0) / (TP + FP)\n",
    "Recall = (TP * 1.0) / (TP + FN)\n",
    "print(\"F1 \", (2*Precision*Recall)/(Precision+Recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One should get accuracy and F1 measure higher than 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other option is to use a surpevised classifier, as suggested:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate all three datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.concatenate((testset_bad, train_set, testset_good))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make target labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "yBad = [1] * testset_bad.shape[0]\n",
    "yGood = [0] * (train_set.shape[0] + testset_good.shape[0])\n",
    "y = yBad + yGood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "divide data onto training set and test set. 60% in traing set, 40% in test set\n",
    "There are 25065 samples of anomalous http requests, all datasets contain together 97065 samples,\n",
    "40% of all data means 38826.0 samples. So in both datasets, training and test set will be samples from both classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train, X_test, y_train, y_test = train_test_split(all_data, y, test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a surpevised logistic regression classifier:\n",
    "Logistic regression is a simple classifier, but it can provide us a baseline and also give us an information\n",
    "whether the classification problem is linearly separable or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgs = LogisticRegression()\n",
    "lgs.fit(X_train, y_train)\n",
    "y_pred = lgs.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ACC, F1] = get_acc_and_f1(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show accuracy and F1 measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9964456807294081  F1: 0.993103448275862\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy: \", ACC, \" F1:\", F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the accuracy and F1 are over 0.99, it indicates, that the two classes are linearly separable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With SVM using a linear kernel, it is possible to get 100% accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.LinearSVC()\n",
    "parameters = {'C': [0.001, 0.1, 1,  100, 1000]}\n",
    "clf = GridSearchCV(svc, parameters, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune the hyperparameter C with 10-fold crossvalidation using grid search algorithm\n",
    "and classify the samples in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ACC, F1] = get_acc_and_f1(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  1.0  F1: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy: \", ACC, \" F1:\", F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM: The primal classification problem is transformed onto a dual problem (Lagrangian duality).\n",
    "The dual problem is a convex optimization problem and leads to Quadratic Programming.\n",
    "For a QP problems, it is always possible to find a global optima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
